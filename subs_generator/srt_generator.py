#!/usr/bin/env python3
import os
import time
import whisper
import re
from datetime import datetime
from difflib import SequenceMatcher
import sys
import torch
import warnings

# Supprimer les warnings normaux (RTX 4000 + PyTorch)
warnings.filterwarnings("ignore", category=UserWarning, module="whisper.timing")
warnings.filterwarnings("ignore", category=FutureWarning, module="whisper")
warnings.filterwarnings("ignore", message=".*weights_only.*")

def setup_rtx4000_model():
    """Configuration optimale pour Quadro RTX 4000 avec mod√®le MEDIUM."""
    print("üéÆ Configuration GPU RTX 4000...")
    
    # V√©rifier et pr√©parer le GPU
    if not torch.cuda.is_available():
        print("   ‚ùå CUDA non disponible - utilisation CPU")
        return whisper.load_model("medium", device="cpu")
    
    try:
        # Nettoyer la m√©moire GPU
        torch.cuda.empty_cache()
        
        # Informations GPU
        gpu_props = torch.cuda.get_device_properties(0)
        total_memory = gpu_props.total_memory / 1024**3
        
        print(f"   GPU d√©tect√©: {gpu_props.name}")
        print(f"   M√©moire totale: {total_memory:.1f} GB")
        
        # Test GPU basique
        test_tensor = torch.randn(100, 100).cuda()
        result = torch.matmul(test_tensor, test_tensor)
        torch.cuda.synchronize()
        
        print("   ‚úÖ Test GPU r√©ussi")
        
        # Le mod√®le medium n√©cessite moins de m√©moire (~1.5GB vs ~3GB)
        if total_memory < 3:  # Moins de 3GB
            print("   ‚ö†Ô∏è M√©moire GPU limit√©e mais suffisante pour le mod√®le medium")
        
        # Charger le mod√®le medium sur GPU
        print("   üì• Chargement mod√®le 'medium' sur GPU...")
        model = whisper.load_model("medium", device="cuda")
        
        # V√©rifier m√©moire utilis√©e
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        print(f"   ‚úÖ Mod√®le medium charg√© - M√©moire utilis√©e: {allocated:.1f} GB")
        print(f"   üöÄ Avantages medium: ~2-3x plus rapide, moins d'hallucinations")
        
        return model
        
    except Exception as e:
        print(f"   ‚ùå Erreur GPU: {e}")
        print("   üîÑ Fallback vers CPU...")
        return whisper.load_model("medium", device="cpu")

def get_rtx4000_transcribe_params(model_device):
    """Param√®tres √âQUILIBR√âS pour mod√®le MEDIUM - Optimis√©s pour vitesse et pr√©cision."""
    print("üéØ Configuration √âQUILIBR√âE pour mod√®le MEDIUM...")
    
    base_params = {
        "language": "fr",
        "verbose": False,
        "word_timestamps": True,
        "temperature": 0.0,  # D√©terministe pour √©viter la cr√©ativit√©
        
        # PARAM√àTRES √âQUILIBR√âS - Le mod√®le medium est naturellement moins hallucinatoire
        "no_speech_threshold": 0.4,    # L√©g√®rement plus sensible que large (0.5)
        "logprob_threshold": -1.8,     # L√©g√®rement plus permissif que large (-1.5)
        "compression_ratio_threshold": 3.0,  # L√©g√®rement plus permissif que large (2.8)
        
        # R√©duire les inf√©rences mais garder un peu de contexte
        "condition_on_previous_text": True,   # ‚úÖ ACTIV√â mais avec prompt strict
        "initial_prompt": "Transcription pr√©cise en fran√ßais. Ne pas inventer de contenu."
        # Pas de suppress_tokens - laisser le mod√®le fonctionner naturellement
    }
    
    # Optimisations GPU avec param√®tres mod√©r√©s
    if model_device == "cuda":
        base_params.update({
            "fp16": True,
            "beam_size": 3,  # Compromis entre 1 et 5
            "patience": 1.5  # Compromis entre 1.0 et 2.0
        })
        print("   üöÄ GPU + Medium: fp16=True, beam_size=3, patience=1.5")
    else:
        print("   üíª CPU + Medium activ√©")
    
    print("   üìä Seuils optimis√©s pour medium:")
    print(f"      - no_speech_threshold: {base_params['no_speech_threshold']} (plus sensible)")
    print(f"      - logprob_threshold: {base_params['logprob_threshold']} (plus permissif)")
    print(f"      - compression_ratio: {base_params['compression_ratio_threshold']} (plus permissif)")
    print(f"      - beam_size: {base_params.get('beam_size', 'N/A')} (compromis)")
    print("   ‚öñÔ∏è Medium: Naturellement moins d'hallucinations + Plus rapide")
    print("   üîÑ Tokens naturels: Aucune suppression pour comportement pr√©visible")
    
    return base_params

def analyze_transcription_coverage(result, input_audio):
    """Analyser la couverture de transcription pour d√©tecter les trous."""
    
    # Obtenir dur√©e audio totale
    ffprobe_cmd = [
        "ffprobe", "-v", "error", "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1", input_audio
    ]
    
    try:
        import subprocess
        duration_result = subprocess.run(ffprobe_cmd, capture_output=True, text=True, timeout=30)
        if duration_result.returncode == 0:
            audio_duration = float(duration_result.stdout.strip())
        else:
            print("‚ö†Ô∏è Impossible d'obtenir la dur√©e audio")
            return
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur analyse dur√©e: {e}")
        return
    
    # Analyser couverture
    segments = result.get("segments", [])
    if not segments:
        print("‚ùå AUCUN segment transcrit!")
        return
    
    # Calculer temps total transcrit
    total_transcribed = sum(seg["end"] - seg["start"] for seg in segments)
    coverage_percent = (total_transcribed / audio_duration) * 100
    
    print(f"\nüìä ANALYSE COUVERTURE TRANSCRIPTION (MOD√àLE MEDIUM):")
    print(f"   üïê Dur√©e audio totale: {audio_duration:.1f}s ({audio_duration/60:.1f} min)")
    print(f"   üé§ Temps transcrit: {total_transcribed:.1f}s ({total_transcribed/60:.1f} min)")
    print(f"   üìà Couverture: {coverage_percent:.1f}%")
    
    # D√©tecter les trous significatifs
    gaps = []
    for i in range(len(segments) - 1):
        gap_duration = segments[i+1]["start"] - segments[i]["end"]
        if gap_duration > 3.0:  # Trous de plus de 3 secondes
            gaps.append({
                "start": segments[i]["end"],
                "end": segments[i+1]["start"], 
                "duration": gap_duration
            })
    
    if gaps:
        print(f"   ‚ö†Ô∏è {len(gaps)} TROUS SIGNIFICATIFS d√©tect√©s:")
        for i, gap in enumerate(gaps[:5]):  # Afficher 5 premiers trous
            start_min = gap["start"] / 60
            end_min = gap["end"] / 60
            print(f"      {i+1}. {start_min:.1f}min ‚Üí {end_min:.1f}min (trou: {gap['duration']:.1f}s)")
        
        if len(gaps) > 5:
            print(f"      ... et {len(gaps) - 5} autres trous")
    else:
        print("   ‚úÖ Aucun trou significatif d√©tect√©")
    
    # Recommandations sp√©cifiques au mod√®le medium
    if coverage_percent < 70:
        print("\nüö® COUVERTURE FAIBLE! Recommandations pour mod√®le medium:")
        print("   1. V√©rifiez la qualit√© de votre audio")
        print("   2. Augmentez le volume si n√©cessaire") 
        print("   3. R√©duisez le bruit de fond")
        print("   4. Le mod√®le medium peut manquer la parole tr√®s faible")
    elif coverage_percent < 85:
        print("\n‚ö†Ô∏è Couverture mod√©r√©e - normal pour le mod√®le medium")
    else:
        print("\n‚úÖ Excellente couverture pour le mod√®le medium!")
    
    return coverage_percent

def normalize_text(text):
    """Normalise le texte pour la comparaison."""
    # Enlever la ponctuation et les espaces multiples
    text = re.sub(r'[^\w\s]', '', text.lower())
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def advanced_deduplication(segments):
    """D√©duplication contre les r√©p√©titions de Whisper."""
    if not segments:
        return []
    
    print("D√©duplication des r√©p√©titions en cours...")
    clean_segments = []
    
    i = 0
    while i < len(segments):
        current = segments[i]
        current_text = current["text"].strip()
        
        # Ignorer les segments tr√®s courts ou vides
        if len(current_text) < 3:
            i += 1
            continue
        
        # Chercher des r√©p√©titions cons√©cutives (minimum 3 r√©p√©titions)
        consecutive_count = 1
        j = i + 1
        
        while j < len(segments):
            next_text = segments[j]["text"].strip()
            similarity = SequenceMatcher(None, 
                                       normalize_text(current_text), 
                                       normalize_text(next_text)).ratio()
            
            if similarity > 0.8:  # Tr√®s similaire
                consecutive_count += 1
                j += 1
            else:
                break
        
        # Si on a trouv√© 3+ r√©p√©titions cons√©cutives, c'est probablement une r√©p√©tition
        if consecutive_count >= 3:
            print(f"  üîÑ R√©p√©tition d√©tect√©e ({consecutive_count} fois): '{current_text[:40]}...'")
            # Garder seulement la premi√®re occurrence avec dur√©e √©tendue
            extended_segment = {
                "start": current["start"],
                "end": segments[j-1]["end"] if j-1 < len(segments) else current["end"],
                "text": current_text,
                "words": current.get("words", [])
            }
            clean_segments.append(extended_segment)
            i = j  # Passer apr√®s toutes les r√©p√©titions
        else:
            # Segment normal, le garder
            clean_segments.append(current)
            i += 1
    
    return clean_segments

def merge_compound_words(word_data):
    """Merge compound words with apostrophes and hyphens into single linguistic units."""
    if not word_data:
        return []
    
    merged_words = []
    i = 0
    
    while i < len(word_data):
        current_word = word_data[i]
        word_text = current_word["word"].strip()
        
        # Check if we should merge with the next word
        should_merge = False
        
        if i < len(word_data) - 1:
            next_word = word_data[i + 1]
            next_text = next_word["word"].strip()
            
            # Pattern 1: Current word ends with apostrophe or hyphen
            # Example: ["j'", "aime"] or ["Saint-", "Esprit"]
            if word_text.endswith("'") or word_text.endswith("-"):
                should_merge = True
            
            # Pattern 2: Next word starts with apostrophe or hyphen  
            # Example: ["l", "'Esprit"] or ["Saint", "-Esprit"] or ["s", "'apprend"]
            elif next_text.startswith("'") or next_text.startswith("-"):
                should_merge = True
        
        if should_merge:
            next_word = word_data[i + 1]
            next_text = next_word["word"].strip()
            
            # Merge the compound word
            merged_text = word_text + next_text
            merged_word = {
                "word": " " + merged_text if current_word["word"].startswith(" ") else merged_text,
                "start": current_word["start"],
                "end": next_word["end"]
            }
            merged_words.append(merged_word)
            i += 2  # Skip both words since we merged them
        else:
            merged_words.append(current_word)
            i += 1
    
    return merged_words

def count_linguistic_words(word_data):
    """Count actual linguistic words after merging compound words."""
    merged_words = merge_compound_words(word_data)
    return len(merged_words)

def process_words_sequentially(word_data, max_words, max_chars, min_gap):
    """Traite les mots s√©quentiellement avec timings pr√©cis - AUCUN mot perdu."""
    if not word_data:
        return []
    
    # First, merge compound words to get proper linguistic units
    merged_word_data = merge_compound_words(word_data)
    
    segments = []
    current_words = []
    
    for i, word_info in enumerate(merged_word_data):
        current_words.append(word_info)
        
        # Construire le texte actuel avec les espaces de Whisper
        current_text = "".join([w["word"] for w in current_words]).strip()
        
        # V√©rifier si on doit cr√©er un segment (now using linguistic word count)
        should_create_segment = (
            len(current_words) >= max_words or  # Limite de mots linguistiques atteinte
            len(current_text) >= max_chars or   # Limite de caract√®res atteinte
            i == len(merged_word_data) - 1 or   # Dernier mot
            word_info["word"].rstrip().endswith(('.', '!', '?', ';', ':'))  # Fin de phrase naturelle
        )
        
        if should_create_segment and current_text:
            # Cr√©er le segment avec timings pr√©cis
            segment_start = current_words[0]["start"]
            segment_end = current_words[-1]["end"]
            
            segments.append({
                "start": segment_start,
                "end": segment_end,
                "text": current_text
            })
            
            print(f"    Segment cr√©√©: '{current_text}' ({len(current_words)} mots linguistiques, {len(current_text)} chars)")
            
            # R√©initialiser pour le prochain segment
            current_words = []
    
    return segments

def process_text_sequentially(text, start_time, end_time, max_words, max_chars, min_gap):
    """Traite le texte s√©quentiellement sans horodatages pr√©cis - AUCUN mot perdu."""
    words = text.split()
    if not words:
        return []
    
    segments = []
    duration = end_time - start_time
    total_words = len(words)
    
    word_index = 0
    
    while word_index < total_words:
        current_words = []
        
        # Prendre des mots jusqu'aux limites
        while (word_index < total_words and 
               len(current_words) < max_words and 
               len(" ".join(current_words + [words[word_index]])) <= max_chars):
            current_words.append(words[word_index])
            word_index += 1
        
        # Si aucun mot n'a pu √™tre ajout√© (mot tr√®s long), le prendre quand m√™me
        if not current_words and word_index < total_words:
            current_words.append(words[word_index])
            word_index += 1
        
        if current_words:
            # Calculer les timings proportionnels
            words_start_ratio = (word_index - len(current_words)) / total_words
            words_end_ratio = word_index / total_words
            
            segment_start = start_time + (duration * words_start_ratio)
            segment_end = start_time + (duration * words_end_ratio)
            
            current_text = " ".join(current_words)
            
            segments.append({
                "start": segment_start,
                "end": segment_end,
                "text": current_text
            })
            
            print(f"    Segment cr√©√©: '{current_text}' ({len(current_words)} mots, {len(current_text)} chars)")
    
    return segments

def smart_segmentation(segments):
    """Segmentation s√©quentielle - Version 27 am√©lior√©e avec z√©ro perte de mots."""
    print("Segmentation s√©quentielle intelligente en cours...")
    optimized = []
    
    # Param√®tres √©prouv√©s de la version 27
    MAX_WORDS_PER_SEGMENT = 5  # 5 mots maximum
    MAX_CHARS_PER_SEGMENT = 35  # 35 caract√®res maximum  
    MAX_DURATION = 2.8  # 2.8 secondes maximum
    MIN_GAP = 0.1
    
    for segment in segments:
        text = segment["text"].strip()
        start_time = segment["start"]
        end_time = segment["end"]
        duration = end_time - start_time
        
        # Count linguistic words (after merging compound words)
        if "words" in segment and segment["words"]:
            linguistic_word_count = count_linguistic_words(segment["words"])
        else:
            # Fallback: split on whitespace for basic word count
            linguistic_word_count = len(text.split())
        
        # Si le segment respecte d√©j√† les crit√®res, le garder tel quel (logique version 27)
        if (linguistic_word_count <= MAX_WORDS_PER_SEGMENT and 
            len(text) <= MAX_CHARS_PER_SEGMENT and 
            duration <= MAX_DURATION):
            optimized.append({
                "start": start_time,
                "end": end_time,
                "text": text
            })
            continue
        
        # Segment d√©passe les limites : traitement s√©quentiel avec timings pr√©cis
        print(f"  Traitement s√©quentiel: '{text[:30]}...' ({linguistic_word_count} mots linguistiques)")
        
        if "words" in segment and segment["words"] and len(segment["words"]) > 0:
            # Utiliser les horodatages pr√©cis des mots (version 27 style)
            word_data = segment["words"]
            processed_segments = process_words_sequentially(word_data, MAX_WORDS_PER_SEGMENT, MAX_CHARS_PER_SEGMENT, MIN_GAP)
            optimized.extend(processed_segments)
        else:
            # Fallback : traitement s√©quentiel sans horodatages pr√©cis
            fallback_segments = process_text_sequentially(text, start_time, end_time, MAX_WORDS_PER_SEGMENT, MAX_CHARS_PER_SEGMENT, MIN_GAP)
            optimized.extend(fallback_segments)
    
    return optimized

def fill_gaps(segments):
    """Comble les trous entre segments pour √©viter les silences dans les sous-titres."""
    if len(segments) < 2:
        return segments
    
    print("V√©rification et comblement des trous...")
    filled_segments = []
    
    for i, segment in enumerate(segments):
        filled_segments.append(segment)
        
        # V√©rifier s'il y a un trou avec le segment suivant
        if i < len(segments) - 1:
            next_segment = segments[i + 1]
            gap = next_segment["start"] - segment["end"]
            
            # Si le trou est significatif (> 1 seconde) mais pas trop grand (< 5 secondes)
            if 1.0 < gap < 5.0:
                print(f"  Trou d√©tect√© de {gap:.1f}s entre segments {i+1} et {i+2}")
                # √âtendre l√©g√®rement le segment actuel pour r√©duire le trou
                segment["end"] = min(segment["end"] + gap/2, next_segment["start"] - 0.1)
    
    return filled_segments

def resolve_overlaps(segments):
    """R√©sout les chevauchements temporels entre segments."""
    print("R√©solution des chevauchements temporels...")
    if not segments:
        return []
    
    # Trier les segments par heure de d√©but
    segments.sort(key=lambda x: x["start"])
    
    resolved = []
    for i, current in enumerate(segments):
        if i == 0:
            resolved.append(current)
            continue
        
        previous = resolved[-1]
        
        # V√©rifier s'il y a chevauchement
        if current["start"] < previous["end"]:
            # Ajuster les horaires pour √©viter le chevauchement
            gap = 0.1  # 100ms de gap minimum
            previous["end"] = current["start"] - gap
            
            # S'assurer que le segment pr√©c√©dent n'est pas trop court
            if previous["end"] - previous["start"] < 0.5:
                previous["end"] = previous["start"] + 0.5
                current["start"] = previous["end"] + gap
            
            print(f"  Chevauchement r√©solu entre segments {i} et {i+1}")
        
        resolved.append(current)
    
    return resolved

def write_srt(segments, file):
    """√âcrire les segments au format SRT - TOUS les mots sont pr√©serv√©s."""
    segment_count = 0
    for segment in segments:
        text = segment["text"].strip()
        
        # Garder tous les segments avec du texte
        if text:
            segment_count += 1
            
            # Num√©ro de s√©quence
            file.write(f"{segment_count}\n")
            
            # Temps d√©but --> fin
            start = format_timestamp(segment["start"])
            end = format_timestamp(segment["end"])
            file.write(f"{start} --> {end}\n")
            
            # Texte complet - aucune troncature
            file.write(f"{text}\n\n")

def format_timestamp(seconds):
    """Convertir secondes en format HH:MM:SS,mmm pour SRT."""
    hours = int(seconds / 3600)
    seconds %= 3600
    minutes = int(seconds / 60)
    seconds %= 60
    milliseconds = int((seconds - int(seconds)) * 1000)
    
    return f"{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}"

def generate_srt(input_audio_path, output_srt_path=None):
    """
    Fonction principale qui g√©n√®re le fichier SRT professionnel.
    Compatible avec le pipeline existant.
    
    Args:
        input_audio_path: Chemin vers le fichier audio d'entr√©e
        output_srt_path: Chemin o√π sauvegarder le fichier SRT (optionnel)
    
    Returns:
        str: Chemin du fichier SRT g√©n√©r√©
    """
    # V√©rifier si le fichier existe
    if not os.path.exists(input_audio_path):
        raise FileNotFoundError(f"Erreur: Le fichier {input_audio_path} n'existe pas.")
    
    print("=" * 60)
    print("G√âN√âRATEUR SRT PROFESSIONNEL - MOD√àLE MEDIUM")
    print("=" * 60)
    print(f"D√©but de la transcription avec le mod√®le Whisper MEDIUM optimis√©...")
    start_time = time.time()
    
    # Charger le mod√®le avec optimisations RTX 4000
    model = setup_rtx4000_model()
    model_device = "cuda" if model.device.type == "cuda" else "cpu"
    
    # Param√®tres optimis√©s selon le device
    transcribe_params = get_rtx4000_transcribe_params(model_device)
    
    # Transcription avec param√®tres anti-r√©p√©tition optimis√©s
    print("üé§ D√©but transcription avec mod√®le MEDIUM...")
    try:
        result = model.transcribe(input_audio_path, **transcribe_params)
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"‚úÖ Transcription MEDIUM termin√©e en {duration:.2f} secondes.")
        
        # Monitoring GPU si utilis√©
        if model_device == "cuda":
            max_memory = torch.cuda.max_memory_allocated(0) / 1024**3
            print(f"üéÆ M√©moire GPU max utilis√©e: {max_memory:.1f} GB")
            
    except Exception as e:
        print(f"‚ùå Erreur transcription: {e}")
        raise
    
    # Analyser la couverture de transcription
    print("\nüìä Analyse de la couverture...")
    coverage = analyze_transcription_coverage(result, input_audio_path)
    
    print("\n" + "=" * 60)
    print("√âTAPE 1: D√âDUPLICATION INTELLIGENTE")
    print("=" * 60)
    
    # D√©duplication intelligente (fusionner au lieu de supprimer)
    cleaned_segments = advanced_deduplication(result["segments"])
    print(f"Apr√®s d√©duplication intelligente: {len(cleaned_segments)} segments")
    
    print("\n" + "=" * 60)
    print("√âTAPE 2: OPTIMISATION DES SEGMENTS")
    print("=" * 60)
    
    # Optimiser les segments pour une meilleure lisibilit√©
    optimized_segments = smart_segmentation(cleaned_segments)
    print(f"Segments optimis√©s: {len(optimized_segments)} segments")
    
    print("\n" + "=" * 60)
    print("√âTAPE 3: COMBLEMENT DES TROUS")
    print("=" * 60)
    
    # Combler les trous potentiels
    filled_segments = fill_gaps(optimized_segments)
    print(f"Apr√®s comblement des trous: {len(filled_segments)} segments")
    
    # V√©rification finale des chevauchements
    final_segments = resolve_overlaps(filled_segments)
    print(f"Apr√®s r√©solution des chevauchements: {len(final_segments)} segments")
    
    # D√©terminer le nom du fichier de sortie
    if output_srt_path:
        srt_filename = output_srt_path
        # Cr√©er le r√©pertoire de sortie si n√©cessaire
        os.makedirs(os.path.dirname(srt_filename), exist_ok=True)
    else:
        # Cr√©er le dossier output s'il n'existe pas
        os.makedirs("output", exist_ok=True)
        # G√©n√©rer un timestamp pour le nom du fichier
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        srt_filename = f"output/reference_audio_medium_{timestamp}.srt"
    
    # G√©n√©rer le fichier SRT
    with open(srt_filename, "w", encoding="utf-8") as srt_file:
        write_srt(final_segments, srt_file)
    
    # Nettoyage final GPU
    if model_device == "cuda":
        torch.cuda.empty_cache()
        print("üßπ M√©moire GPU nettoy√©e")
    
    print("\n" + "=" * 60)
    print("G√âN√âRATION SRT TERMIN√âE AVEC SUCC√àS!")
    print("=" * 60)
    print(f"‚ö° Performance RTX 4000 + MEDIUM: Transcription en {duration:.2f}s")
    print(f"üìÅ Fichier SRT professionnel g√©n√©r√©: {srt_filename}")
    print(f"üìä Nombre total de segments: {len(final_segments)}")
    
    # Statistiques finales
    if final_segments:
        total_duration = final_segments[-1]["end"]
        print(f"üïê Dur√©e totale trait√©e: {total_duration:.1f} secondes ({total_duration/60:.1f} min)")
    
    print("‚ú® Qualit√©: Timings pr√©cis + Aucun mot perdu + Segmentation intelligente + Anti-hallucination")
    
    return srt_filename

def main():
    """Fonction principale qui g√©n√®re le fichier SRT professionnel (mode ligne de commande)."""
    # G√©n√©rer un timestamp pour le nom du fichier
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Fichier audio d'entr√©e
    input_audio = "reference_audio.mp3"
    
    # Appeler la fonction g√©n√©rique
    generate_srt(input_audio)

if __name__ == "__main__":
    main()